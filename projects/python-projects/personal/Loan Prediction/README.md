# Loan Prediction Project

This project is part of my personal learning journey in machine learning and data analysis using Python. The goal was to explore common classification algorithms and become familiar with the full machine learning pipeline â€” from data preprocessing to model evaluation â€” using a real-world use case.

## ğŸ“ Files Included

- `1.x-*.ipynb`: A series of Jupyter Notebooks experimenting with different models such as Logistic Regression, Decision Trees, Random Forests, Support Vector Machines, and Linear Regression.
- `LoanDataset - LoansDataset.csv`: The dataset used for training, testing, and validating models.

## ğŸ“Š Project Overview

The use case focuses on predicting loan approval outcomes based on applicant data such as income, employment status, and loan amount. This is a simplified binary classification task.

This project was designed to:
- Practice applying core machine learning models.
- Get hands-on experience with exploratory data analysis (EDA).
- Develop an understanding of data preprocessing techniques.
- Learn basic model tuning and evaluation.

While the structure may not be production-ready, the notebooks reflect an honest and exploratory approach to learning data science.

## ğŸ§ª Models Explored

Each versioned notebook (e.g., `1.0`, `1.1`) represents iterations or refinements. Models implemented:

- **Logistic Regression**
- **Decision Tree**
- **Random Forest**
- **Support Vector Machine (SVM)**
- **Linear Regression** (used for comparison despite the classification objective)

## âš™ï¸ How to Run

1. Open any `.ipynb` file using [Google Colab](https://colab.research.google.com/) or Jupyter Notebook.
2. Ensure the CSV file (`LoanDataset - LoansDataset.csv`) is located in the same directory.
3. Run the notebook from top to bottom to execute the full analysis.

## ğŸ› ï¸ Tools & Libraries Used

- `pandas`
- `numpy`
- `scikit-learn`
- `matplotlib`
- `seaborn`

---

### ğŸ”– Notes

This project was a personal exercise in understanding machine learning basics and working with tabular data. It helped me build confidence in preprocessing techniques, handling missing values, encoding categorical variables, and interpreting model outputs.
