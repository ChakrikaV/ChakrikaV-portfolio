{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4zwzawORteR"
      },
      "source": [
        "Table of Contents\n",
        "1. Useful libraries have been imported\n",
        "2. Zip files are uploaded in and read as dataframes\n",
        "3. Some features are dropped (EDA on different notebook helped decide which features to drop)\n",
        "4. Datasets are merged together\n",
        "5. Categorical features are converted into numerical features\n",
        "6. Missing values/null values are filled in\n",
        "7. Data is standardized\n",
        "8. PCA is performed\n",
        "9. 70/15/15 split test is applied\n",
        "10. Logistic Regression\n",
        "11. Decision Tree\n",
        "12. Random Forest\n",
        "13. Artificial Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "B0ifqxeFSOi3"
      },
      "outputs": [],
      "source": [
        "#Importing useful libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "import os\n",
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#import torch.optim as optim\n",
        "#from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "#import tensorflow as tf\n",
        "#from tensorflow.keras.models import Sequential\n",
        "#from tensorflow.keras.layers import Dense, Dropout\n",
        "#from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yU9tptqq80st"
      },
      "outputs": [],
      "source": [
        "# List function (zip_file_paths) that stores the path to multiple zip files\n",
        "zip_file_paths = [\n",
        "    '/content/POS_CASH_balance.csv.zip',\n",
        "    '/content/application_test.csv.zip',\n",
        "    '/content/application_train.csv.zip',\n",
        "    '/content/bureau.csv.zip',\n",
        "    '/content/bureau_balance.csv.zip',\n",
        "    '/content/credit_card_balance.csv.zip',\n",
        "    '/content/installments_payments.csv.zip',\n",
        "    '/content/previous_application.csv.zip'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vYH-N7JZ8_y2"
      },
      "outputs": [],
      "source": [
        "# This creates a folder where all the zipfiles will be stored within a directory\n",
        "zipfiles = 'zipfiles'\n",
        "os.makedirs(zipfiles, exist_ok=True) #exist_ok=True states that if this code is run again and directory already exhists to ignore it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "U71HX63l93Nf",
        "outputId": "05c024d7-7acc-403c-ad8c-77df5bedcc4f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/POS_CASH_balance.csv.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3618786896.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This creates a continious loop to go through all 8 path files and extract them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mzip_file_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip_file_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m    \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m        \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Extracted Zip Files: {zip_file_path}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/zipfile/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/POS_CASH_balance.csv.zip'"
          ]
        }
      ],
      "source": [
        "# This creates a continious loop to go through all 8 path files and extract them\n",
        "for zip_file_path in zip_file_paths:\n",
        "   with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "       zip_ref.extractall(zipfiles)\n",
        "   print(f'Extracted Zip Files: {zip_file_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNjL75XODmKS"
      },
      "outputs": [],
      "source": [
        "# This creates an empty dictionary so that when it loops through the zipfiles it converst them into a dataframe\n",
        "dataframes = {}\n",
        "for file in os.listdir(zipfiles):\n",
        "    if file.endswith('.csv'):\n",
        "        df_name = file.split('.')[0] # This is taking the name of the dataframe before the .\n",
        "        dataframes[df_name] = pd.read_csv(os.path.join(zipfiles, file)) # This allows for the code to read this as the name before the .\n",
        "        print(f'Read {file} into DataFrame: {df_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_7zlSXgEPpe"
      },
      "outputs": [],
      "source": [
        "# Renaming the dataframes into new variable names\n",
        "credit_card_balance = dataframes['credit_card_balance']\n",
        "previous_application = dataframes['previous_application']\n",
        "bureau_balance = dataframes['bureau_balance']\n",
        "POS_CASH_balance = dataframes['POS_CASH_balance']\n",
        "application_train = dataframes['application_train']\n",
        "application_test = dataframes['application_test']\n",
        "installments_payments = dataframes['installments_payments']\n",
        "bureau = dataframes['bureau']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EeTLXarEUmm"
      },
      "outputs": [],
      "source": [
        "# EDA was done in a different notebook when decided which features to drop\n",
        "# New variable names are created for each dataset without the dropped variables\n",
        "credit1 = credit_card_balance.drop(columns=[\n",
        "    'SK_ID_PREV', 'AMT_PAYMENT_CURRENT', 'CNT_DRAWINGS_POS_CURRENT',\n",
        "    'AMT_RECEIVABLE_PRINCIPAL', 'AMT_RECIVABLE', 'AMT_DRAWINGS_ATM_CURRENT',\n",
        "    'CNT_DRAWINGS_ATM_CURRENT', 'CNT_DRAWINGS_OTHER_CURRENT'\n",
        "])\n",
        "\n",
        "bureau1 = bureau.drop(columns=[\n",
        "    'CREDIT_DAY_OVERDUE', 'CNT_CREDIT_PROLONG'\n",
        "])\n",
        "\n",
        "install1 = installments_payments.drop(columns=[\n",
        "    'DAYS_ENTRY_PAYMENT', 'AMT_INSTALMENT', 'NUM_INSTALMENT_VERSION',\n",
        "    'NUM_INSTALMENT_NUMBER'\n",
        "])\n",
        "\n",
        "POS1 = POS_CASH_balance.drop(columns=[\n",
        "    'CNT_INSTALMENT_FUTURE'\n",
        "])\n",
        "\n",
        "pre_app1 = previous_application.drop(columns=[\n",
        "    'DAYS_LAST_DUE', 'NFLAG_LAST_APPL_IN_DAY', 'RATE_INTEREST_PRIMARY',\n",
        "    'AMT_GOODS_PRICE', 'AMT_APPLICATION'\n",
        "])\n",
        "\n",
        "app_train1 = application_train.drop(columns=[\n",
        "    'REGION_POPULATION_RELATIVE', 'FLAG_MOBIL', 'FLAG_DOCUMENT_2',\n",
        "    'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_10',\n",
        "    'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19',\n",
        "    'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
        "    'OBS_60_CNT_SOCIAL_CIRCLE', 'LIVINGAREA_MEDI', 'LIVINGAPARTMENTS_MEDI',\n",
        "    'ELEVATORS_MEDI', 'APARTMENTS_MEDI', 'LIVINGAREA_MODE',\n",
        "    'LIVINGAPARTMENTS_MODE', 'ELEVATORS_MODE', 'APARTMENTS_MODE',\n",
        "    'LIVINGAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'ELEVATORS_AVG',\n",
        "    'APARTMENTS_AVG', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI',\n",
        "    'LANDAREA_MEDI', 'FLOORSMIN_MEDI', 'FLOORSMAX_MEDI',\n",
        "    'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI',\n",
        "    'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ENTRANCES_MEDI',\n",
        "    'FLOORSMIN_MODE', 'LANDAREA_MODE', 'FLOORSMAX_MODE',\n",
        "    'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE',\n",
        "    'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ENTRANCES_MODE',\n",
        "    'LIVE_CITY_NOT_WORK_CITY', 'LIVE_REGION_NOT_WORK_REGION',\n",
        "    'REGION_RATING_CLIENT', 'CNT_CHILDREN', 'AMT_GOODS_PRICE',\n",
        "    'NONLIVINGAPARTMENTS_MODE', 'FLAG_DOCUMENT_15','NONLIVINGAPARTMENTS_AVG',\n",
        "    'FLAG_CONT_MOBILE', 'DAYS_ID_PUBLISH', 'BASEMENTAREA_AVG'\n",
        "])\n",
        "\n",
        "app_test1 = application_test.drop(columns=[\n",
        "    'REGION_POPULATION_RELATIVE', 'FLAG_MOBIL', 'FLAG_DOCUMENT_2',\n",
        "    'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_10',\n",
        "    'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19',\n",
        "    'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
        "    'OBS_60_CNT_SOCIAL_CIRCLE', 'LIVINGAREA_MEDI', 'LIVINGAPARTMENTS_MEDI',\n",
        "    'ELEVATORS_MEDI', 'APARTMENTS_MEDI', 'LIVINGAREA_MODE',\n",
        "    'LIVINGAPARTMENTS_MODE', 'ELEVATORS_MODE', 'APARTMENTS_MODE',\n",
        "    'LIVINGAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'ELEVATORS_AVG',\n",
        "    'APARTMENTS_AVG', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI',\n",
        "    'LANDAREA_MEDI', 'FLOORSMIN_MEDI', 'FLOORSMAX_MEDI',\n",
        "    'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI',\n",
        "    'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ENTRANCES_MEDI',\n",
        "    'FLOORSMIN_MODE', 'LANDAREA_MODE', 'FLOORSMAX_MODE',\n",
        "    'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE',\n",
        "    'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ENTRANCES_MODE',\n",
        "    'LIVE_CITY_NOT_WORK_CITY', 'LIVE_REGION_NOT_WORK_REGION',\n",
        "    'REGION_RATING_CLIENT', 'CNT_CHILDREN', 'AMT_GOODS_PRICE',\n",
        "    'NONLIVINGAPARTMENTS_MODE', 'FLAG_DOCUMENT_15','NONLIVINGAPARTMENTS_AVG',\n",
        "    'FLAG_CONT_MOBILE', 'DAYS_ID_PUBLISH', 'BASEMENTAREA_AVG'\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEIPzKhbJGqN"
      },
      "outputs": [],
      "source": [
        "app_train1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t35Tf6IxJG0C"
      },
      "outputs": [],
      "source": [
        "app_test1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMXC589fJPWh"
      },
      "outputs": [],
      "source": [
        "# This combines the app_train1 and app_test1 dataframes into one variable dataframe\n",
        "combined_df = pd.concat([app_train1, app_test1], ignore_index=True, sort=False)\n",
        "# ignore_index=True helps avoid duplicate indexes\n",
        "# sort=False keeps the columns in order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DftVr291JTgM"
      },
      "outputs": [],
      "source": [
        "combined_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo4jLb0aJmYq"
      },
      "outputs": [],
      "source": [
        "bureau1.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrOjNCOsJmkt"
      },
      "outputs": [],
      "source": [
        "bureau_balance.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KugckMNAJ_8a"
      },
      "outputs": [],
      "source": [
        "bureau_balance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WXdYJ-hKAEU"
      },
      "outputs": [],
      "source": [
        "bureau1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0FiPZs9bQhA"
      },
      "outputs": [],
      "source": [
        "# groups the dataframe bureau_balance by the feature SK_ID_BUREAU (agg is for summary)\n",
        "bb_agg = bureau_balance.groupby('SK_ID_BUREAU').agg({\n",
        "    'MONTHS_BALANCE': 'count',\n",
        "    'STATUS': lambda x: (x == '0').sum()\n",
        "}).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYiPW9gKbZOU"
      },
      "outputs": [],
      "source": [
        "# groups the dataframe bureau by the feature SK_ID_CURR (agg is for summary)\n",
        "bureau_agg = bureau.groupby('SK_ID_CURR').agg({\n",
        "    'AMT_CREDIT_SUM': 'sum',\n",
        "    'AMT_CREDIT_SUM_DEBT': 'sum',\n",
        "    'AMT_CREDIT_SUM_LIMIT': 'sum',\n",
        "    'CREDIT_ACTIVE': lambda x: (x == 'Active').sum(),\n",
        "    'AMT_CREDIT_MAX_OVERDUE': 'max',\n",
        "}).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmLoppOfKEhx"
      },
      "outputs": [],
      "source": [
        "# merged bureau_agg and bb_agg into the dataframe merged_bureau\n",
        "merged_bureau = pd.merge(bureau_agg, bb_agg, left_on='SK_ID_CURR', right_on='SK_ID_BUREAU', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KtBG-BmKHwE"
      },
      "outputs": [],
      "source": [
        "merged_bureau.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGJxj7Oicilh"
      },
      "outputs": [],
      "source": [
        "# merged combined_df and merged_bureau into a new dataframe named final_df on SK_ID_CURR\n",
        "final_df = pd.merge(combined_df, merged_bureau, on='SK_ID_CURR', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMyreZCmckV6"
      },
      "outputs": [],
      "source": [
        "final_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDO9Dh_adHUZ"
      },
      "outputs": [],
      "source": [
        "credit1.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UMrUqzgdHXi"
      },
      "outputs": [],
      "source": [
        "POS1.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obQ32VhudHaw"
      },
      "outputs": [],
      "source": [
        "pre_app1.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMcz2dUhdHdl"
      },
      "outputs": [],
      "source": [
        "install1.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXl6wvrxdHgu"
      },
      "outputs": [],
      "source": [
        "# groups the dataframe credit1 by the SK_ID_CURR (agg is for summary)\n",
        "credit1_agg = credit1.groupby('SK_ID_CURR').agg({\n",
        "    'MONTHS_BALANCE': 'count',\n",
        "    'AMT_BALANCE': 'sum',\n",
        "    'AMT_CREDIT_LIMIT_ACTUAL': 'sum',\n",
        "    'AMT_DRAWINGS_CURRENT': 'sum',\n",
        "    'AMT_DRAWINGS_OTHER_CURRENT': 'sum',\n",
        "    'AMT_DRAWINGS_POS_CURRENT': 'sum',\n",
        "    'AMT_PAYMENT_TOTAL_CURRENT': 'sum',\n",
        "    'CNT_DRAWINGS_CURRENT': 'sum',\n",
        "    'NAME_CONTRACT_STATUS': lambda x: x.value_counts().get('Active', 0)\n",
        "}).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-kgzCTQeiZK"
      },
      "outputs": [],
      "source": [
        "# groups the dataframe POS1 by the SK_ID_CURR (agg is for summary)\n",
        "POS1_agg = POS1.groupby('SK_ID_CURR').agg({\n",
        "    'MONTHS_BALANCE': 'count',\n",
        "    'CNT_INSTALMENT': 'sum',\n",
        "    'NAME_CONTRACT_STATUS': lambda x: x.value_counts().get('Active', 0)\n",
        "}).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-cSp98_eicp"
      },
      "outputs": [],
      "source": [
        "# groups the dataframe pre_app1 by the SK_ID_CURR (agg is for summary)\n",
        "pre_app1_agg = pre_app1.groupby('SK_ID_CURR').agg({\n",
        "    'SK_ID_PREV': 'count',\n",
        "    'AMT_ANNUITY': 'sum',\n",
        "    'AMT_CREDIT': 'sum',\n",
        "    'AMT_DOWN_PAYMENT': 'sum',\n",
        "    'NAME_CONTRACT_TYPE': lambda x: x.value_counts().idxmax(),\n",
        "}).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySdc-SMeeifb"
      },
      "outputs": [],
      "source": [
        "# groups the dataframe install1 by the SK_ID_CURR (agg is for summary)\n",
        "install1_agg = install1.groupby('SK_ID_CURR').agg({\n",
        "    'SK_ID_PREV': 'count',\n",
        "    'AMT_PAYMENT': 'sum'\n",
        "}).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqKTwMnYfXmH"
      },
      "outputs": [],
      "source": [
        "# merging credit1_agg to the overall final dataset (the apps and bureau datasets)\n",
        "final_df = final_df.merge(credit1_agg, on='SK_ID_CURR', how='left')\n",
        "final_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL6AWSeQfXug"
      },
      "outputs": [],
      "source": [
        "# merging POS1_agg to the overall final dataset\n",
        "final_df = final_df.merge(POS1_agg, on='SK_ID_CURR', how='left')\n",
        "final_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mssIeOBReiin"
      },
      "outputs": [],
      "source": [
        "# merging pre_app1_agg to the overall final dataset\n",
        "final_df = final_df.merge(pre_app1_agg, on='SK_ID_CURR', how='left')\n",
        "final_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trSJI_2keimG"
      },
      "outputs": [],
      "source": [
        "# merging install1_agg to the overall final dataset\n",
        "final_df = final_df.merge(install1_agg, on='SK_ID_CURR', how='left')\n",
        "final_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATIjUjvZfzDJ"
      },
      "outputs": [],
      "source": [
        "# checks for mixed columns\n",
        "mixed_type_cols_final = []\n",
        "for col in final_df.columns: #Checks all columns in a loop\n",
        "    if final_df[col].dtype == 'object': # sees if it is an object\n",
        "        if final_df[col].apply(type).nunique() > 1: # checks to see if there is more than one datatype in a column\n",
        "            mixed_type_cols_final.append(col) # adds all the found columns in a list\n",
        "\n",
        "print(\"Columns with mixed types in final_df:\", mixed_type_cols_final) # prints the list of mixed columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgQcB3PWgGTJ"
      },
      "outputs": [],
      "source": [
        "# converting the mixed columsn into a numerical datatype\n",
        "for col in mixed_type_cols_final:\n",
        "    final_df[col] = pd.to_numeric(final_df[col], errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMtMwqEigGWX"
      },
      "outputs": [],
      "source": [
        "# this is doing one hot encoding- converting the categorical features\n",
        "final_df = pd.get_dummies(final_df, drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFc2uBiDqJgu"
      },
      "outputs": [],
      "source": [
        "final_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGn9CieUfy__"
      },
      "outputs": [],
      "source": [
        "# filling in missing data with 0\n",
        "final_df.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thisis ="
      ],
      "metadata": {
        "id": "T_OXQDqHg-4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_variable = final_df['TARGET']"
      ],
      "metadata": {
        "id": "f6gYWEWqNfzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(target_variable)"
      ],
      "metadata": {
        "id": "eH4xvzpxNh4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variances = final_df.var()\n",
        "sorted_variances = variances.sort_values(ascending=False)\n",
        "top_20_features = sorted_variances.head(20)\n",
        "print(top_20_features)"
      ],
      "metadata": {
        "id": "P-vtPAGli2kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_20_feature_names = top_20_features.index.tolist()\n",
        "top_20_feature_names.append(target_variable)  # Add the 'TARGET' column name to the list\n",
        "top_20_df = final_df[top_20_feature_names]\n",
        "top_20_df.head()"
      ],
      "metadata": {
        "id": "A_TPNLiE2yQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target column name as a string\n",
        "target_variable = 'TARGET'  # This is the column name, not the Series itself\n",
        "\n",
        "# Get the list of top 20 feature names from the variance calculation\n",
        "top_20_feature_names = top_20_features.index.tolist()\n",
        "\n",
        "# Add the target column name ('TARGET') to the list of top 20 features\n",
        "top_20_feature_names.append(target_variable)\n",
        "\n",
        "# Create a new DataFrame with the top 20 features and the target column\n",
        "top_20_df = final_df[top_20_feature_names]\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "top_20_df.head()\n"
      ],
      "metadata": {
        "id": "X0iN_wy3Pe9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "top_20_df.to_csv('/content/20df.csv', index=False)\n",
        "top_20_df.head()\n",
        "os.path.exists('/content/20df.csv')\n",
        "files.download('/content/20df.csv')"
      ],
      "metadata": {
        "id": "k_6zHB6_j7hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = top_20_df.sample(n=1000, random_state=42)"
      ],
      "metadata": {
        "id": "etQedI_Bc_sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample.shape"
      ],
      "metadata": {
        "id": "VnxeQahIdLyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample.to_csv('/content/sample21.csv', index=False)\n",
        "os.path.exists('/content/sample21.csv')\n",
        "files.download('/content/sample21.csv')"
      ],
      "metadata": {
        "id": "dS0vMO-KdO7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guP4mA81QxXp"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(\n",
        "    n_samples=12,         # 12 rows of data\n",
        "    n_features=173,       # 173 features\n",
        "    n_informative=3,      # 3 informative features\n",
        "    n_redundant=0,        # No redundant features\n",
        "    n_repeated=0,         # No repeated features\n",
        "    n_classes=2,          # Binary classification (0 and 1)\n",
        "    random_state=42       # Ensures reproducibility\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnrbrmwPQ-vF"
      },
      "outputs": [],
      "source": [
        "feature_columns = [f'Feature_{i+1}' for i in range(X.shape[1])]\n",
        "\n",
        "# Convert to DataFrame\n",
        "sample_df = pd.DataFrame(X, columns=feature_columns)\n",
        "sample_df['Target'] = y\n",
        "\n",
        "sample_df.head(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgP1GzxufzGL"
      },
      "outputs": [],
      "source": [
        "# Identify the target column and seperating it before standardization\n",
        "X = final_df.drop(columns=['TARGET'])\n",
        "y = final_df['TARGET']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5sn4Un5-bf8"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7O6_4n3gGZT"
      },
      "outputs": [],
      "source": [
        "# standardizing all the columns but the TARGET column\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "be6yFAKc9y15"
      },
      "outputs": [],
      "source": [
        "print(X_scaled[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j2il0YxiGHD"
      },
      "outputs": [],
      "source": [
        "final_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUtiIqYbhxQ6"
      },
      "outputs": [],
      "source": [
        "# applying PCA for dimentionality reduction\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEDMiDRThxUJ"
      },
      "outputs": [],
      "source": [
        "# applying transformation\n",
        "pca_transformed = pca.transform(X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFzldcN2hxXQ"
      },
      "outputs": [],
      "source": [
        "# creating a new dataframe for the transformed pca features\n",
        "pca_df = pd.DataFrame(data=pca_transformed, columns=[f'Principal Component {i+1}' for i in range(pca_transformed.shape[1])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNm2G4b-UaJf"
      },
      "outputs": [],
      "source": [
        "pca_components_df = pd.DataFrame(pca.components_, columns=X.columns, index=[f'Principal Component {i+1}' for i in range(pca_transformed.shape[1])])\n",
        "pca_components_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MgJssDqhxae"
      },
      "outputs": [],
      "source": [
        "pca_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTFWh78YhxdP"
      },
      "outputs": [],
      "source": [
        "pca_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyK5JY16mSkk"
      },
      "outputs": [],
      "source": [
        "# reestablishing the X and y\n",
        "X = pca_df\n",
        "y = final_df['TARGET']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPjQvXSfwA6v"
      },
      "outputs": [],
      "source": [
        "# Doing a 70/15/15 split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHeVu1nvwBES"
      },
      "outputs": [],
      "source": [
        "log_model = LogisticRegression()\n",
        "log_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLzapRGkwFv4"
      },
      "outputs": [],
      "source": [
        "y_test_pred = log_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlXDJedpwH5u"
      },
      "outputs": [],
      "source": [
        "y_val_pred = log_model.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egJKItqwwK8u"
      },
      "outputs": [],
      "source": [
        "scores = cross_val_score(log_model, X, y, cv=5)\n",
        "print(\"Cross-Validation Scores:\", scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFhzkTY-wfqk"
      },
      "outputs": [],
      "source": [
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvOub4s_wfwb"
      },
      "outputs": [],
      "source": [
        "y_test_pred = dt_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(\"Test Accuracy (Decision Tree):\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZtMUfd9wfz5"
      },
      "outputs": [],
      "source": [
        "y_val_pred = dt_model.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(\"Validation Accuracy (Decision Tree):\", val_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeiOnnnxwf3L"
      },
      "outputs": [],
      "source": [
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyB828KPyhd2"
      },
      "outputs": [],
      "source": [
        "y_test_pred = rf_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(\"Test Accuracy (Random Forest):\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFOWzW_KyhhY"
      },
      "outputs": [],
      "source": [
        "y_val_pred = rf_model.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(\"Validation Accuracy (Random Forest):\", val_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhDEuBwG_lmm"
      },
      "outputs": [],
      "source": [
        "X_val.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target column name as a string\n",
        "target_variable = 'TARGET'  # This is the column name, not the Series itself\n",
        "\n",
        "target_column = final_df[target_variable]\n",
        "X_val_with_target = X_val.copy()  # Make a copy of X_val to avoid modifying it in place\n",
        "X_val_with_target[target_variable] = target_column\n",
        "X_val_with_target.head()\n"
      ],
      "metadata": {
        "id": "6IJsoTxPxqJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "X_val_with_target.to_csv('/content/validation.csv', index=False)\n",
        "X_val_with_target.head()\n",
        "os.path.exists('/content/validation.csv')\n",
        "files.download('/content/validation.csv')"
      ],
      "metadata": {
        "id": "t52Ncj6AzlCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBUYfYdEvSb9"
      },
      "outputs": [],
      "source": [
        "nn_model = MLPClassifier(random_state=42, max_iter=300)\n",
        "nn_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBklkSQxvSkA"
      },
      "outputs": [],
      "source": [
        "y_test_pred = nn_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(\"Test Accuracy (Neural Network):\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnsVYq6Uxc6i"
      },
      "outputs": [],
      "source": [
        "y_val_pred = nn_model.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(\"Validation Accuracy (Neural Network):\", val_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXE_2zlXZRzH"
      },
      "outputs": [],
      "source": [
        "ann_model = Sequential()\n",
        "ann_model.add(Dense(units=128, activation='relu', input_dim=X_train.shape[1]))  # Input layer\n",
        "ann_model.add(Dropout(0.2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf8epjpUNAQ5"
      },
      "outputs": [],
      "source": [
        "ann_model.add(Dense(units=64, activation='relu'))\n",
        "ann_model.add(Dropout(0.2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoysYbe6nW85"
      },
      "outputs": [],
      "source": [
        "ann_model.add(Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIvU_69_nXBR"
      },
      "outputs": [],
      "source": [
        "ann_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HQ1AiW6JcoTm"
      },
      "outputs": [],
      "source": [
        "ann_model.fit(X_train, y_train, epochs=50, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTNg42Q7NAUv"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = ann_model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YsZZfn6n6Gq"
      },
      "outputs": [],
      "source": [
        "y_val_pred = ann_model.predict(X_val)\n",
        "y_val_pred = (y_val_pred > 0.5)  # Convert probabilities to binary (0 or 1)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO7vDYU67f_s"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Confusion Matrix for Random Forest Predictions')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn-ekdT7xdBR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Get classification report\n",
        "report = classification_report(y_test, y_test_pred, target_names=['Negative', 'Positive'])\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQozSr7N4dWF"
      },
      "outputs": [],
      "source": [
        "prediction_df = pd.DataFrame({\n",
        "    'Actual': y_test,\n",
        "    'Predicted': y_test_pred\n",
        "})\n",
        "\n",
        "# Plot the actual vs predicted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='Actual', hue='Predicted', data=prediction_df)\n",
        "plt.title('Actual vs Predicted Target for Random Forest')\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da9kKROg4dZJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_test_pred, color='blue', alpha=0.5)\n",
        "plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Line where Actual = Predicted\n",
        "plt.title('Predicted vs Actual Values')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXPNHVNXqlAx"
      },
      "outputs": [],
      "source": [
        "prediction_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNJiORUDeDjl"
      },
      "outputs": [],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKm0ZLaOeDmc"
      },
      "outputs": [],
      "source": [
        "final_df['TARGET'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3H4mFuKeDp3"
      },
      "outputs": [],
      "source": [
        "prediction_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPnyJcDuFvZg"
      },
      "outputs": [],
      "source": [
        "X_test.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REzjO-fC0VXo"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(\n",
        "    n_samples=12,         # 12 rows of data\n",
        "    n_features=173,         # 3 features\n",
        "    n_informative=3,      # All 3 features are informative\n",
        "    n_redundant=0,        # No redundant features\n",
        "    n_repeated=0,         # No repeated features\n",
        "    n_classes=2,          # Binary classification (0 and 1)\n",
        "    random_state=42       # Ensures reproducibility\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyMe3mt3VxQb"
      },
      "outputs": [],
      "source": [
        "feature_columns = [f'Feature_{i+1}' for i in range(X.shape[1])]\n",
        "sample_df = pd.DataFrame(X, columns=feature_columns)\n",
        "sample_df['Target'] = y\n",
        "\n",
        "sample_df.head(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKJRJYERVxTX"
      },
      "outputs": [],
      "source": [
        "unknown_data = final_df.sample(n=10, random_state=42)\n",
        "unknown_data.to_csv('new_data.csv', index=False)\n",
        "unknown_data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jsf1gL57gKyo"
      },
      "outputs": [],
      "source": [
        "unknown_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GL-VubBNgSXY"
      },
      "outputs": [],
      "source": [
        "unknown_data.fillna(0, inplace=True)\n",
        "X_unknown = unknown_data.drop(columns=['TARGET'], errors='ignore')\n",
        "X_unknown_scaled = scaler.transform(X_unknown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi_3FPosgSak"
      },
      "outputs": [],
      "source": [
        "X_unknown_pca = pca.transform(X_unknown_scaled)\n",
        "X_unknown_pca_df = pd.DataFrame(X_unknown_pca, columns=[f'Principal Component {i+1}' for i in range(X_unknown_pca.shape[1])])\n",
        "y_unknown_pred = rf_model.predict(X_unknown_pca_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQDNEV68gSeR"
      },
      "outputs": [],
      "source": [
        "unknown_data['Predicted_Target'] = y_unknown_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WOuqEkcgShd"
      },
      "outputs": [],
      "source": [
        "cols = ['SK_ID_CURR', 'TARGET', 'Predicted_Target'] + [col for col in unknown_data.columns if col not in ['SK_ID_CURR', 'TARGET', 'Predicted_Target']]\n",
        "unknown_data = unknown_data[cols]\n",
        "unknown_data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0hLFBl2meE8"
      },
      "outputs": [],
      "source": [
        "sample_data = final_df.sample(n=100, random_state=42)\n",
        "sample_data.to_csv('/content/sample_data.csv', index=False)\n",
        "sample_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfhKV0kknZhq"
      },
      "outputs": [],
      "source": [
        "os.path.exists('/content/sample_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdzsKBUanBSu"
      },
      "outputs": [],
      "source": [
        "files.download('/content/sample_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4okJgFubmpHg"
      },
      "outputs": [],
      "source": [
        "sample_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVy4S_L9DdcY"
      },
      "outputs": [],
      "source": [
        "print(unknown_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiJL3efOCMa4"
      },
      "outputs": [],
      "source": [
        "y_true = unknown_data['TARGET']\n",
        "y_pred = unknown_data['Predicted_Target']\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Confusion Matrix for Predicted Unknown Data')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIFYidRsEjaa"
      },
      "outputs": [],
      "source": [
        "print(final_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l4GZSI7FGDv"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(data=final_df, x='TARGET')\n",
        "plt.title('Bar Chart of Target Values in Original Dataset')\n",
        "plt.xlabel('TARGET')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(ticks=[0, 1], labels=['Non-Fraudulent (0.0)', 'Fraudulent (1.0)'], rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaNy10peFhgc"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(data=unknown_data, x='TARGET')\n",
        "plt.title('Bar Chart of Target Values in Unknown Dataset')\n",
        "plt.xlabel('TARGET')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(ticks=[0, 1], labels=['Non-Fraudulent (0.0)', 'Fraudulent (1.0)'], rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ENlEokTJbGZ"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)  # Show all columns\n",
        "pd.set_option('display.max_rows', None)  # Optionally show all rows (be cautious with large datasets)\n",
        "\n",
        "# Access the 4th row (index 3)\n",
        "row_4 = unknown_data.iloc[3]\n",
        "print(row_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvkgOzGV2bjL"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "joblib.dump(log_model, 'logistic_regression_model.pkl')\n",
        "joblib.dump(dt_model, 'decision_tree_model.pkl')\n",
        "joblib.dump(rf_model, 'random_forest_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slNkiQe-2kwy"
      },
      "outputs": [],
      "source": [
        "ann_model.save('ann_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdQDXhFh2pZU"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('logistic_regression_model.pkl')\n",
        "files.download('decision_tree_model.pkl')\n",
        "files.download('random_forest_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlHYR94U3BAS"
      },
      "outputs": [],
      "source": [
        "files.download('ann_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xXBC1_ln4GEp"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WtrbndC3Ps_"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrFhndyS65Sp"
      },
      "outputs": [],
      "source": [
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "joblib.dump(pca, 'pca.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekM4wwa4ZKvO"
      },
      "outputs": [],
      "source": [
        "files.download('scaler.pkl')\n",
        "files.download('pca.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gssq_vtj_LkO"
      },
      "outputs": [],
      "source": [
        "log_model = joblib.load('logistic_regression_model.pkl')\n",
        "dt_model = joblib.load('decision_tree_model.pkl')\n",
        "rf_model = joblib.load('random_forest_model.pkl')\n",
        "ann_model = load_model('ann_model.h5')\n",
        "\n",
        "# Load scaler and PCA (assuming they were saved earlier)\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "pca = joblib.load('pca.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8IH9lyeEYT1"
      },
      "outputs": [],
      "source": [
        "def data_source():\n",
        "    # Show information about data and allow file upload\n",
        "    st.title('Data Source')\n",
        "    st.write(\"This section loads the raw data from CSV files.\")\n",
        "\n",
        "    # You can use your original code for extracting data\n",
        "    # Assuming `dataframes` dictionary contains the loaded data\n",
        "    zipfiles = 'zipfiles'\n",
        "    dataframes = {}\n",
        "    for file in os.listdir(zipfiles):\n",
        "        if file.endswith('.csv'):\n",
        "            df_name = file.split('.')[0]\n",
        "            dataframes[df_name] = pd.read_csv(os.path.join(zipfiles, file))\n",
        "\n",
        "    # Show a sample of the data\n",
        "    if dataframes:\n",
        "        for df_name, df in dataframes.items():\n",
        "            st.write(f\"### {df_name} Data\")\n",
        "            st.dataframe(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDATl0IPEek_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def eda():\n",
        "    st.title('Exploratory Data Analysis (EDA)')\n",
        "\n",
        "    # Assuming that these datasets are loaded already (adjust paths or variables if needed)\n",
        "    # Apply the feature drops for each dataset\n",
        "    credit1 = credit_card_balance.drop(columns=[\n",
        "       'SK_ID_PREV', 'AMT_PAYMENT_CURRENT', 'CNT_DRAWINGS_POS_CURRENT',\n",
        "       'AMT_RECEIVABLE_PRINCIPAL', 'AMT_RECIVABLE', 'AMT_DRAWINGS_ATM_CURRENT',\n",
        "       'CNT_DRAWINGS_ATM_CURRENT', 'CNT_DRAWINGS_OTHER_CURRENT'\n",
        "    ])\n",
        "\n",
        "    bureau1 = bureau.drop(columns=[\n",
        "       'CREDIT_DAY_OVERDUE', 'CNT_CREDIT_PROLONG'\n",
        "    ])\n",
        "\n",
        "    install1 = installments_payments.drop(columns=[\n",
        "       'DAYS_ENTRY_PAYMENT', 'AMT_INSTALMENT', 'NUM_INSTALMENT_VERSION',\n",
        "       'NUM_INSTALMENT_NUMBER'\n",
        "    ])\n",
        "\n",
        "    POS1 = POS_CASH_balance.drop(columns=[\n",
        "       'CNT_INSTALMENT_FUTURE'\n",
        "    ])\n",
        "\n",
        "    pre_app1 = previous_application.drop(columns=[\n",
        "       'DAYS_LAST_DUE', 'NFLAG_LAST_APPL_IN_DAY', 'RATE_INTEREST_PRIMARY',\n",
        "       'AMT_GOODS_PRICE', 'AMT_APPLICATION'\n",
        "    ])\n",
        "\n",
        "    app_train1 = application_train.drop(columns=[\n",
        "       'REGION_POPULATION_RELATIVE', 'FLAG_MOBIL', 'FLAG_DOCUMENT_2',\n",
        "       'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_10',\n",
        "       'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19',\n",
        "       'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
        "       'OBS_60_CNT_SOCIAL_CIRCLE', 'LIVINGAREA_MEDI', 'LIVINGAPARTMENTS_MEDI',\n",
        "       'ELEVATORS_MEDI', 'APARTMENTS_MEDI', 'LIVINGAREA_MODE',\n",
        "       'LIVINGAPARTMENTS_MODE', 'ELEVATORS_MODE', 'APARTMENTS_MODE',\n",
        "       'LIVINGAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'ELEVATORS_AVG',\n",
        "       'APARTMENTS_AVG', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI',\n",
        "       'LANDAREA_MEDI', 'FLOORSMIN_MEDI', 'FLOORSMAX_MEDI',\n",
        "       'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI',\n",
        "       'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ENTRANCES_MEDI',\n",
        "       'FLOORSMIN_MODE', 'LANDAREA_MODE', 'FLOORSMAX_MODE',\n",
        "       'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE',\n",
        "       'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ENTRANCES_MODE',\n",
        "       'LIVE_CITY_NOT_WORK_CITY', 'LIVE_REGION_NOT_WORK_REGION',\n",
        "       'REGION_RATING_CLIENT', 'CNT_CHILDREN', 'AMT_GOODS_PRICE',\n",
        "       'NONLIVINGAPARTMENTS_MODE', 'FLAG_DOCUMENT_15', 'NONLIVINGAPARTMENTS_AVG',\n",
        "       'FLAG_CONT_MOBILE', 'DAYS_ID_PUBLISH', 'BASEMENTAREA_AVG'\n",
        "    ])\n",
        "\n",
        "    app_test1 = application_test.drop(columns=[\n",
        "       'REGION_POPULATION_RELATIVE', 'FLAG_MOBIL', 'FLAG_DOCUMENT_2',\n",
        "       'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_10',\n",
        "       'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19',\n",
        "       'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
        "       'OBS_60_CNT_SOCIAL_CIRCLE', 'LIVINGAREA_MEDI', 'LIVINGAPARTMENTS_MEDI',\n",
        "       'ELEVATORS_MEDI', 'APARTMENTS_MEDI', 'LIVINGAREA_MODE',\n",
        "       'LIVINGAPARTMENTS_MODE', 'ELEVATORS_MODE', 'APARTMENTS_MODE',\n",
        "       'LIVINGAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'ELEVATORS_AVG',\n",
        "       'APARTMENTS_AVG', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI',\n",
        "       'LANDAREA_MEDI', 'FLOORSMIN_MEDI', 'FLOORSMAX_MEDI',\n",
        "       'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI',\n",
        "       'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ENTRANCES_MEDI',\n",
        "       'FLOORSMIN_MODE', 'LANDAREA_MODE', 'FLOORSMAX_MODE',\n",
        "       'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE',\n",
        "       'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ENTRANCES_MODE',\n",
        "       'LIVE_CITY_NOT_WORK_CITY', 'LIVE_REGION_NOT_WORK_REGION',\n",
        "       'REGION_RATING_CLIENT', 'CNT_CHILDREN', 'AMT_GOODS_PRICE',\n",
        "       'NONLIVINGAPARTMENTS_MODE', 'FLAG_DOCUMENT_15','NONLIVINGAPARTMENTS_AVG',\n",
        "       'FLAG_CONT_MOBILE', 'DAYS_ID_PUBLISH', 'BASEMENTAREA_AVG'\n",
        "    ])\n",
        "\n",
        "    # Step 1: Combine train and test data\n",
        "    combined_df = pd.concat([app_train1, app_test1], ignore_index=True, sort=False)\n",
        "\n",
        "    # Step 2: Aggregations for different datasets (Bureau, Credit, POS, Pre-app, Install)\n",
        "    bb_agg = bureau_balance.groupby('SK_ID_BUREAU').agg({\n",
        "       'MONTHS_BALANCE': 'count',\n",
        "       'STATUS': lambda x: (x == '0').sum()\n",
        "    }).reset_index()\n",
        "\n",
        "    bureau_agg = bureau1.groupby('SK_ID_CURR').agg({\n",
        "       'AMT_CREDIT_SUM': 'sum',\n",
        "       'AMT_CREDIT_SUM_DEBT': 'sum',\n",
        "       'AMT_CREDIT_SUM_LIMIT': 'sum',\n",
        "       'CREDIT_ACTIVE': lambda x: (x == 'Active').sum(),\n",
        "       'AMT_CREDIT_MAX_OVERDUE': 'max',\n",
        "    }).reset_index()\n",
        "\n",
        "    # Merging bureau datasets (bureau balance with bureau)\n",
        "    merged_bureau = pd.merge(bureau_agg, bb_agg, left_on='SK_ID_CURR', right_on='SK_ID_BUREAU', how='left')\n",
        "\n",
        "    # Step 3: Merge final_df with the merged bureau data\n",
        "    final_df = pd.merge(combined_df, merged_bureau, on='SK_ID_CURR', how='left')\n",
        "\n",
        "    # Step 4: Perform aggregations on other datasets (credit1, POS1, pre_app1, install1)\n",
        "    credit1_agg = credit1.groupby('SK_ID_CURR').agg({\n",
        "       'MONTHS_BALANCE': 'count',\n",
        "       'AMT_BALANCE': 'sum',\n",
        "       'AMT_CREDIT_LIMIT_ACTUAL': 'sum',\n",
        "       'AMT_DRAWINGS_CURRENT': 'sum',\n",
        "       'AMT_DRAWINGS_OTHER_CURRENT': 'sum',\n",
        "       'AMT_DRAWINGS_POS_CURRENT': 'sum',\n",
        "       'AMT_PAYMENT_TOTAL_CURRENT': 'sum',\n",
        "       'CNT_DRAWINGS_CURRENT': 'sum',\n",
        "       'NAME_CONTRACT_STATUS': lambda x: x.value_counts().get('Active', 0)\n",
        "    }).reset_index()\n",
        "\n",
        "    POS1_agg = POS1.groupby('SK_ID_CURR').agg({\n",
        "       'MONTHS_BALANCE': 'count',\n",
        "       'CNT_INSTALMENT': 'sum',\n",
        "       'NAME_CONTRACT_STATUS': lambda x: x.value_counts().get('Active', 0)\n",
        "    }).reset_index()\n",
        "\n",
        "    pre_app1_agg = pre_app1.groupby('SK_ID_CURR').agg({\n",
        "       'SK_ID_PREV': 'count',\n",
        "       'AMT_ANNUITY': 'sum',\n",
        "       'AMT_CREDIT': 'sum',\n",
        "       'AMT_DOWN_PAYMENT': 'sum',\n",
        "       'NAME_CONTRACT_TYPE': lambda x: x.value_counts().idxmax(),\n",
        "    }).reset_index()\n",
        "\n",
        "    install1_agg = install1.groupby('SK_ID_CURR').agg({\n",
        "       'SK_ID_PREV': 'count',\n",
        "       'AMT_PAYMENT': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Merging aggregated dataframes into final_df\n",
        "    final_df = final_df.merge(credit1_agg, on='SK_ID_CURR', how='left')\n",
        "    final_df = final_df.merge(POS1_agg, on='SK_ID_CURR', how='left')\n",
        "    final_df = final_df.merge(pre_app1_agg, on='SK_ID_CURR', how='left')\n",
        "    final_df = final_df.merge(install1_agg, on='SK_ID_CURR', how='left')\n",
        "\n",
        "    # Step 5: Handle Mixed-Type Columns\n",
        "    mixed_type_cols_final = []\n",
        "    for col in final_df.columns:\n",
        "        if final_df[col].dtype == 'object':\n",
        "            if final_df[col].apply(type).nunique() > 1:\n",
        "                mixed_type_cols_final.append(col)\n",
        "\n",
        "    # Print out mixed columns\n",
        "    st.write(\"Columns with mixed types in final_df:\", mixed_type_cols_final)\n",
        "\n",
        "    # Convert mixed-type columns to numeric\n",
        "    for col in mixed_type_cols_final:\n",
        "        final_df[col] = pd.to_numeric(final_df[col], errors='coerce')\n",
        "\n",
        "    # Step 6: Handle Categorical Features and Get Dummies\n",
        "    final_df = pd.get_dummies(final_df, drop_first=True)\n",
        "\n",
        "    # Step 7: Fill missing values with 0\n",
        "    final_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Show the resulting dataframe after all EDA steps\n",
        "    st.write(\"Data after EDA (merged, cleaned, and processed):\")\n",
        "    st.dataframe(final_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygx2WThoEeu5"
      },
      "outputs": [],
      "source": [
        "def training():\n",
        "    st.title('Model Training')\n",
        "\n",
        "    # Split the data\n",
        "    X = final_df.drop(columns=['TARGET'])\n",
        "    y = final_df['TARGET']\n",
        "\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    pca_transformed = pca.transform(X_scaled)\n",
        "\n",
        "    # Re-define X using PCA-transformed data\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(pca_transformed, y, test_size=0.3, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Train and evaluate models\n",
        "    models = {\n",
        "        \"Logistic Regression\": log_model,\n",
        "        \"Decision Tree\": dt_model,\n",
        "        \"Random Forest\": rf_model,\n",
        "        \"ANN\": ann_model\n",
        "    }\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        st.write(f\"### {model_name}\")\n",
        "\n",
        "        if model_name == 'ANN':\n",
        "            ann_model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
        "            test_loss, test_accuracy = ann_model.evaluate(X_test, y_test)\n",
        "            st.write(f\"Test Accuracy: {test_accuracy}\")\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "            y_test_pred = model.predict(X_test)\n",
        "            test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "            st.write(f\"Test Accuracy: {test_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6fSItWdEpe0"
      },
      "outputs": [],
      "source": [
        "def prediction():\n",
        "    st.title('Prediction')\n",
        "\n",
        "    # Load new data\n",
        "    st.write(\"Upload new data to predict outcomes.\")\n",
        "    uploaded_file = st.file_uploader(\"Choose a file\")\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        new_data = pd.read_csv(uploaded_file)\n",
        "        new_data.fillna(0, inplace=True)\n",
        "\n",
        "        # Apply the same preprocessing pipeline (scaling + PCA)\n",
        "        X_new = new_data.drop(columns=['TARGET'], errors='ignore')\n",
        "        X_new_scaled = scaler.transform(X_new)\n",
        "        X_new_pca = pca.transform(X_new_scaled)\n",
        "\n",
        "        # Choose a model for prediction (e.g., Random Forest)\n",
        "        y_new_pred = rf_model.predict(X_new_pca)\n",
        "        new_data['Predicted_Target'] = y_new_pred\n",
        "\n",
        "        st.write(\"Predicted Data:\")\n",
        "        st.dataframe(new_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZT3K3Ai_PLE"
      },
      "outputs": [],
      "source": [
        "# Sidebar navigation\n",
        "st.sidebar.header(\"Navigation\")\n",
        "\n",
        "# Buttons to navigate between steps\n",
        "if st.sidebar.button(\"Data Source\"):\n",
        "    st.session_state['current_step'] = 'Data Source'\n",
        "\n",
        "if st.sidebar.button(\"EDA\"):\n",
        "    st.session_state['current_step'] = 'EDA'\n",
        "\n",
        "if st.sidebar.button(\"Training\"):\n",
        "    st.session_state['current_step'] = 'Training'\n",
        "\n",
        "if st.sidebar.button(\"Prediction\"):\n",
        "    st.session_state['current_step'] = 'Prediction'\n",
        "\n",
        "# Render the appropriate step based on user choice\n",
        "if st.session_state['current_step'] == 'Data Source':\n",
        "    data_source()\n",
        "elif st.session_state['current_step'] == 'EDA':\n",
        "    eda()\n",
        "elif st.session_state['current_step'] == 'Training':\n",
        "    training()\n",
        "elif st.session_state['current_step'] == 'Prediction':\n",
        "    prediction()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}